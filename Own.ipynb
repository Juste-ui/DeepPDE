{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import math \n",
    "\n",
    "from tensorflow import keras\n",
    "from scipy.stats import norm\n",
    "from numpy.polynomial.hermite import hermgauss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import urllib\n",
    "import os\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "from DeepPDE.Classes.DPDEGenerator  import DPDEGenerator\n",
    "from DeepPDE.Classes.DPEDEModel  import DPDEModel\n",
    "from DeepPDE.Classes.HighwayLayer import *\n",
    "from DeepPDE.tools.others import *\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nr_samples_surface_plot = 21\n",
    "nr_samples_scatter_plot = 1000\n",
    "nr_samples_error_calculation = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters. Re-train model after any changes.\n",
    "s_min_interest = 25\n",
    "s_max_interest = 150\n",
    "t_min_interest = 0.5\n",
    "t_max_interest = 4.\n",
    "\n",
    "riskfree_rate_min = 0.1\n",
    "riskfree_rate_max = 0.3\n",
    "riskfree_rate_eval = 0.2\n",
    "\n",
    "volatility_min = 0.1\n",
    "volatility_max = 0.3\n",
    "volatility1_eval = 0.1\n",
    "volatility2_eval = 0.3\n",
    "\n",
    "correlation_min = 0.2\n",
    "correlation_max = 0.8\n",
    "correlation_eval = 0.5\n",
    "\n",
    "strike_price = 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_nodes_per_layer = 90\n",
    "initial_learning_rate = 0.001\n",
    "localisation_parameter = 1/10.\n",
    "\n",
    "n_train = 100\n",
    "nr_epochs = 601"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension_state = 2\n",
    "dimension_parameter = 4\n",
    "dimension_total = 1 + dimension_state + dimension_parameter\n",
    "\n",
    "t_min = 0.\n",
    "t_max = t_max_interest\n",
    "s_max = strike_price * (1 + 3*volatility_max*t_max)\n",
    "x_max = np.log(s_max)\n",
    "x_min = 2*np.log(strike_price) - x_max\n",
    "\n",
    "normalised_max = 1\n",
    "normalised_min = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalise =transform(0,t_max=t_max, strike_price=strike,volatility_min= volatility_min,\n",
    "                     volatility_max= volatility_max,normalise_min=normalised_min,normalise_max=normalised_max,r_min=riskfree_rate_min,\n",
    "                     r_max= riskfree_rate_max,rho_min= correlation_min,rho_max= correlation_max)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_ab_to_cd(x, a, b, c, d): \n",
    "    \"\"\"\n",
    "    Perform a linear transformation of a scalar from the souce interval\n",
    "    to the target interval.\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- scalar point(s) to transform\n",
    "    a, b -- interval to transform from\n",
    "    c, d -- interval to transform to \n",
    "    \"\"\"\n",
    "    return c + (x-a) * (d-c) / (b-a)\n",
    "\n",
    "def transform_to_logprice(x): \n",
    "    \"\"\" Transform normalised variable to the log-price. \"\"\"\n",
    "    return transform_ab_to_cd(x, normalised_min, normalised_max, x_min, x_max)\n",
    "\n",
    "def transform_to_time(t): \n",
    "    \"\"\" Transform normalised variable to the time variable. \"\"\"\n",
    "    return transform_ab_to_cd(t, normalised_min, normalised_max, t_min, t_max)\n",
    "\n",
    "def normalise_logprice(x):\n",
    "    \"\"\" Transform log-price to its corresponding normalised variable. \"\"\"\n",
    "    return transform_ab_to_cd(x, x_min, x_max, normalised_min, normalised_max)\n",
    "\n",
    "def normalise_time(t): \n",
    "    \"\"\" Transform time to its corresponding normalised variable. \"\"\"\n",
    "    return transform_ab_to_cd(t, t_min, t_max, normalised_min, normalised_max)\n",
    "\n",
    "t_min_interest_normalised = normalise_time(t_min_interest)\n",
    "t_max_interest_normalised = normalise_time(t_max_interest)\n",
    "\n",
    "diff_dx = (normalised_max-normalised_min) / (x_max-x_min) \n",
    "diff_dt = (normalised_max-normalised_min) / (t_max-t_min)\n",
    "\n",
    "def transform_to_riskfree_rate(mu_1):\n",
    "    \"\"\" Transform normalised variable to the risk-free rate. \"\"\"\n",
    "    return transform_ab_to_cd(mu_1, normalised_min, normalised_max,\n",
    "                                    riskfree_rate_min, riskfree_rate_max)\n",
    "\n",
    "def transform_to_volatility(mu_2):\n",
    "    \"\"\" Transform normalised variable to the volatility. \"\"\"\n",
    "    return transform_ab_to_cd(mu_2, normalised_min, normalised_max,\n",
    "                                    volatility_min, volatility_max)\n",
    "    \n",
    "def transform_to_correlation(mu_3):\n",
    "    \"\"\" Transform normalised variable to the correlation. \"\"\"\n",
    "    return transform_ab_to_cd(mu_3, normalised_min, normalised_max,\n",
    "                                    correlation_min, correlation_max)\n",
    "\n",
    "def normalise_riskfree_rate(riskfree_rate):\n",
    "    \"\"\" Transform risk-free rate to its corresponding normalised variable. \"\"\"\n",
    "    return transform_ab_to_cd(riskfree_rate,\n",
    "                              riskfree_rate_min, riskfree_rate_max, \n",
    "                              normalised_min, normalised_max)\n",
    "    \n",
    "def normalise_volatility(volatility):\n",
    "    \"\"\" Transform volatility to its corresponding normalised variable. \"\"\"\n",
    "    return transform_ab_to_cd( volatility, volatility_min, volatility_max,\n",
    "                                            normalised_min, normalised_max)\n",
    "    \n",
    "def normalise_correlation(correlation):\n",
    "    \"\"\" Transform correlation to its corresponding normalised variable. \"\"\"\n",
    "    return transform_ab_to_cd(correlation, correlation_min, correlation_max,\n",
    "                                            normalised_min, normalised_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(inputs):\n",
    "    \"\"\" Creates the neural network by creating three highway layers and an \n",
    "    output layer. Returns the output of these layers as a tensorflow variable.\n",
    "\n",
    "    Keyword arguments:\n",
    "    inputs -- Tensorflow variable of the input layer\n",
    "    \"\"\"\n",
    "    layer0 = keras.layers.Dense(nr_nodes_per_layer, activation=\"tanh\")\n",
    "\n",
    "    layer1 = HighwayLayer(units=nr_nodes_per_layer,\n",
    "                          original_input=dimension_total)\n",
    "    layer2 = HighwayLayer(units=nr_nodes_per_layer,\n",
    "                          original_input=dimension_total)\n",
    "    layer3 = HighwayLayer(units=nr_nodes_per_layer,\n",
    "                          original_input=dimension_total)\n",
    "    \n",
    "    last_layer = keras.layers.Dense(1)\n",
    "\n",
    "    outputs_layer0 = layer0(inputs)\n",
    "    outputs_layer1 = layer1({'previous_layer': outputs_layer0, \n",
    "                             'original_variable': inputs})\n",
    "    outputs_layer2 = layer2({'previous_layer': outputs_layer1, \n",
    "                             'original_variable': inputs})\n",
    "    outputs_layer3 = layer3({'previous_layer': outputs_layer2, \n",
    "                             'original_variable': inputs})\n",
    "\n",
    "    outputs_dnn = last_layer(outputs_layer3)\n",
    "    \n",
    "    inputs_t_normalised = inputs[:, 0:1]\n",
    "    inputs_x1_normalised = inputs[:, 1:2]\n",
    "    inputs_x2_normalised = inputs[:, 2:3]\n",
    "    inputs_p1_normalised = inputs[:, 3:4]\n",
    "    \n",
    "    inputs_t = transform_to_time(inputs_t_normalised)\n",
    "    inputs_x1 = transform_to_logprice(inputs_x1_normalised)\n",
    "    inputs_x2 = transform_to_logprice(inputs_x2_normalised)\n",
    "    inputs_s_mean = (tf.math.exp(inputs_x1) + tf.math.exp(inputs_x2))/2.\n",
    "    riskfree_rate = transform_to_riskfree_rate(inputs_p1_normalised)\n",
    "\n",
    "    localisation = tf.math.log(1+tf.math.exp(localisation_parameter * (\n",
    "            inputs_s_mean - strike_price * tf.exp( - riskfree_rate * inputs_t)\n",
    "              )))/localisation_parameter\n",
    "\n",
    "    return outputs_dnn + localisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPDEModel(keras.Model):\n",
    "    \"\"\" Create a keras model with the deep param. PDE loss function \"\"\"\n",
    "\n",
    "    def train_step(self, data):\n",
    "        \"\"\" Create one optimisation stop based on the deep param. PDE loss function. \"\"\"\n",
    "        data_interior, data_initial = data[0]\n",
    "\n",
    "        riskfree_rate_interior = transform_to_riskfree_rate(data_interior[:, 3:4])\n",
    "        volatility1_interior = transform_to_volatility(data_interior[:, 4:5])\n",
    "        volatility2_interior = transform_to_volatility(data_interior[:, 5:6])\n",
    "        correlation_interior = transform_to_correlation(data_interior[:, 6:7])\n",
    "\n",
    "        x1_initial = transform_to_logprice(data_initial[:, 1:2])\n",
    "        x2_initial = transform_to_logprice(data_initial[:, 2:3])\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            v_interior = self(data_interior, training=True)  # Forward pass\n",
    "            v_initial = self(data_initial, training=True)  # Forward pass bdry\n",
    "\n",
    "            gradient = K.gradients(v_interior, data_interior)[0]\n",
    "\n",
    "            v_dt = diff_dt * gradient[:, 0:1]\n",
    "            v_dx1 = diff_dx * gradient[:, 1:2]\n",
    "            v_dx2 = diff_dx * gradient[:, 2:3]\n",
    "\n",
    "            grad_v_dx1 = K.gradients(v_dx1, data_interior)[0]\n",
    "            grad_v_dx2 = K.gradients(v_dx2, data_interior)[0]\n",
    "\n",
    "            v_dx1dx1 = diff_dx * grad_v_dx1[:, 1:2]\n",
    "            v_dx2dx2 = diff_dx * grad_v_dx2[:, 2:3]\n",
    "            v_dx1dx2 = diff_dx * grad_v_dx1[:, 2:3]\n",
    "            v_dx2dx1 = diff_dx * grad_v_dx2[:, 1:2]\n",
    "\n",
    "            residual_interior = (\n",
    "                v_dt + riskfree_rate_interior * v_interior \n",
    "                - (riskfree_rate_interior - volatility1_interior**2/2) * v_dx1\n",
    "                - (riskfree_rate_interior - volatility2_interior**2/2) * v_dx2\n",
    "                - 0.5 * volatility1_interior**2 * v_dx1dx1\n",
    "                - 0.5 * volatility2_interior**2 * v_dx2dx2 \n",
    "                - 0.5 * correlation_interior \n",
    "                    * volatility1_interior * volatility2_interior * v_dx1dx2 \n",
    "                - 0.5 * correlation_interior \n",
    "                    * volatility2_interior * volatility1_interior * v_dx2dx1\n",
    "                )\n",
    "\n",
    "            s_mean_initial = 0.5 * (\n",
    "                tf.math.exp(x1_initial)+tf.math.exp(x2_initial)) \n",
    "            payoff_initial = K.maximum(s_mean_initial - strike_price, 0)\n",
    "\n",
    "            loss_interior = K.mean(K.square(residual_interior))\n",
    "            loss_initial = K.mean(K.square(v_initial - payoff_initial))\n",
    "            \n",
    "            loss = loss_initial + loss_interior\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        return {\"loss\": loss, \n",
    "                \"loss initial\": loss_initial, \n",
    "                \"loss interior\": loss_interior}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/601\n",
      "10/10 [==============================] - 0s 47ms/step - loss: 0.0077 - loss initial: 0.0073 - loss interior: 4.1423e-04\n",
      "Epoch 2/601\n",
      "10/10 [==============================] - 0s 42ms/step - loss: 0.0015 - loss initial: 0.0015 - loss interior: 7.2757e-06\n",
      "Epoch 3/601\n",
      "10/10 [==============================] - 0s 46ms/step - loss: 6.6272e-04 - loss initial: 6.5970e-04 - loss interior: 3.0209e-06\n",
      "Epoch 4/601\n",
      "10/10 [==============================] - 1s 135ms/step - loss: 2.2451e-04 - loss initial: 2.2210e-04 - loss interior: 2.4163e-06\n",
      "Epoch 5/601\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 7.2221e-05 - loss initial: 7.1869e-05 - loss interior: 3.5211e-07\n",
      "Epoch 6/601\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 2.8391e-05 - loss initial: 2.8223e-05 - loss interior: 1.6745e-07\n",
      "Epoch 7/601\n",
      "10/10 [==============================] - 0s 43ms/step - loss: 1.1042e-05 - loss initial: 1.0984e-05 - loss interior: 5.8051e-08\n",
      "Epoch 8/601\n",
      "10/10 [==============================] - 0s 42ms/step - loss: 4.1588e-06 - loss initial: 4.1347e-06 - loss interior: 2.4074e-08\n",
      "Epoch 9/601\n",
      "10/10 [==============================] - 0s 46ms/step - loss: 1.5323e-06 - loss initial: 1.5238e-06 - loss interior: 8.5283e-09\n",
      "Epoch 10/601\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 5.1328e-07 - loss initial: 5.1001e-07 - loss interior: 3.2749e-09\n",
      "Epoch 11/601\n",
      "10/10 [==============================] - 0s 43ms/step - loss: 1.4987e-07 - loss initial: 1.4919e-07 - loss interior: 6.8274e-10\n",
      "Epoch 12/601\n",
      "10/10 [==============================] - 0s 43ms/step - loss: 5.8736e-08 - loss initial: 5.8316e-08 - loss interior: 4.1958e-10\n",
      "Epoch 13/601\n",
      "10/10 [==============================] - 0s 46ms/step - loss: 2.7108e-08 - loss initial: 2.6958e-08 - loss interior: 1.5030e-10\n",
      "Epoch 14/601\n",
      "10/10 [==============================] - 1s 52ms/step - loss: 6.8326e-09 - loss initial: 6.7828e-09 - loss interior: 4.9734e-11\n",
      "Epoch 15/601\n",
      "10/10 [==============================] - 1s 54ms/step - loss: 3.1487e-09 - loss initial: 3.1372e-09 - loss interior: 1.1538e-11\n",
      "Epoch 16/601\n",
      "10/10 [==============================] - 0s 47ms/step - loss: 9.6435e-10 - loss initial: 9.5910e-10 - loss interior: 5.2459e-12\n",
      "Epoch 17/601\n",
      "10/10 [==============================] - 0s 45ms/step - loss: 4.0324e-10 - loss initial: 4.0037e-10 - loss interior: 2.8734e-12\n",
      "Epoch 18/601\n",
      "10/10 [==============================] - 0s 46ms/step - loss: 1.1452e-10 - loss initial: 1.1369e-10 - loss interior: 8.2875e-13\n",
      "Epoch 19/601\n",
      "10/10 [==============================] - 1s 50ms/step - loss: 5.4096e-11 - loss initial: 5.3805e-11 - loss interior: 2.9081e-13\n",
      "Epoch 20/601\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 1.6015e-11 - loss initial: 1.5929e-11 - loss interior: 8.6055e-14\n",
      "Epoch 21/601\n",
      "10/10 [==============================] - 1s 50ms/step - loss: 5.7162e-12 - loss initial: 5.6891e-12 - loss interior: 2.7109e-14\n",
      "Epoch 22/601\n",
      "10/10 [==============================] - 0s 45ms/step - loss: 2.3665e-12 - loss initial: 2.3571e-12 - loss interior: 9.4519e-15\n",
      "Epoch 23/601\n",
      "10/10 [==============================] - 1s 57ms/step - loss: 7.4006e-13 - loss initial: 7.3769e-13 - loss interior: 2.3730e-15\n",
      "Epoch 24/601\n",
      "10/10 [==============================] - 1s 52ms/step - loss: 2.3043e-13 - loss initial: 2.2942e-13 - loss interior: 1.0125e-15\n",
      "Epoch 25/601\n",
      "10/10 [==============================] - 1s 56ms/step - loss: 1.3016e-13 - loss initial: 1.2974e-13 - loss interior: 4.2043e-16\n",
      "Epoch 26/601\n",
      "10/10 [==============================] - 1s 60ms/step - loss: 4.8869e-14 - loss initial: 4.8686e-14 - loss interior: 1.8306e-16\n",
      "Epoch 27/601\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.6891e-14 - loss initial: 1.6863e-14 - loss interior: 2.8063e-17\n",
      "Epoch 28/601\n",
      "10/10 [==============================] - 1s 51ms/step - loss: 4.7543e-15 - loss initial: 4.7333e-15 - loss interior: 2.0951e-17\n",
      "Epoch 29/601\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 7.0222e-16 - loss initial: 6.8648e-16 - loss interior: 1.5738e-17\n",
      "Epoch 30/601\n",
      "10/10 [==============================] - 0s 36ms/step - loss: 1.0422e-15 - loss initial: 1.0289e-15 - loss interior: 1.3246e-17\n",
      "Epoch 31/601\n",
      "10/10 [==============================] - 0s 46ms/step - loss: 1.8202e-16 - loss initial: 1.7712e-16 - loss interior: 4.8989e-18\n",
      "Epoch 32/601\n",
      "10/10 [==============================] - 0s 45ms/step - loss: 1.4623e-16 - loss initial: 1.4368e-16 - loss interior: 2.5468e-18\n",
      "Epoch 33/601\n",
      "10/10 [==============================] - 1s 53ms/step - loss: 5.5761e-17 - loss initial: 5.4652e-17 - loss interior: 1.1081e-18\n",
      "Epoch 34/601\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 3.5004e-17 - loss initial: 3.4767e-17 - loss interior: 2.3734e-19\n",
      "Epoch 35/601\n",
      "10/10 [==============================] - 1s 54ms/step - loss: 2.8159e-16 - loss initial: 2.7779e-16 - loss interior: 3.8012e-18\n",
      "Epoch 36/601\n",
      "10/10 [==============================] - 0s 50ms/step - loss: 3.7891e-16 - loss initial: 3.7539e-16 - loss interior: 3.5118e-18\n",
      "Epoch 37/601\n",
      "10/10 [==============================] - 0s 34ms/step - loss: 3.4033e-16 - loss initial: 3.3560e-16 - loss interior: 4.7362e-18\n",
      "Epoch 38/601\n",
      "10/10 [==============================] - 1s 56ms/step - loss: 4.7980e-16 - loss initial: 4.7430e-16 - loss interior: 5.4928e-18\n",
      "Epoch 39/601\n",
      "10/10 [==============================] - 0s 31ms/step - loss: 3.4628e-16 - loss initial: 3.4116e-16 - loss interior: 5.1125e-18\n",
      "Epoch 40/601\n",
      "10/10 [==============================] - 0s 47ms/step - loss: 2.8109e-16 - loss initial: 2.7798e-16 - loss interior: 3.1112e-18\n",
      "Epoch 41/601\n",
      "10/10 [==============================] - 0s 48ms/step - loss: 1.8242e-16 - loss initial: 1.8019e-16 - loss interior: 2.2355e-18\n",
      "Epoch 42/601\n",
      "10/10 [==============================] - 0s 41ms/step - loss: 5.5378e-17 - loss initial: 5.3378e-17 - loss interior: 2.0000e-18\n",
      "Epoch 43/601\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 5.8932e-17 - loss initial: 5.8475e-17 - loss interior: 4.5643e-19\n",
      "Epoch 44/601\n",
      "10/10 [==============================] - 0s 50ms/step - loss: 6.1808e-17 - loss initial: 5.9458e-17 - loss interior: 2.3504e-18\n",
      "Epoch 45/601\n",
      "10/10 [==============================] - 0s 32ms/step - loss: 6.1076e-17 - loss initial: 5.9458e-17 - loss interior: 1.6181e-18\n",
      "Epoch 46/601\n",
      "10/10 [==============================] - 0s 33ms/step - loss: 5.8789e-17 - loss initial: 5.7917e-17 - loss interior: 8.7200e-19\n",
      "Epoch 47/601\n",
      "10/10 [==============================] - 0s 33ms/step - loss: 1.4600e-16 - loss initial: 1.4425e-16 - loss interior: 1.7521e-18\n",
      "Epoch 48/601\n",
      "10/10 [==============================] - 0s 43ms/step - loss: 1.0015e-16 - loss initial: 9.9550e-17 - loss interior: 5.9910e-19\n",
      "Epoch 49/601\n",
      "10/10 [==============================] - 0s 31ms/step - loss: 5.8623e-17 - loss initial: 5.7917e-17 - loss interior: 7.0568e-19\n",
      "Epoch 50/601\n",
      "10/10 [==============================] - 1s 61ms/step - loss: 6.0657e-17 - loss initial: 5.7917e-17 - loss interior: 2.7398e-18\n",
      "Epoch 51/601\n",
      "10/10 [==============================] - 0s 45ms/step - loss: 3.3560e-16 - loss initial: 3.3154e-16 - loss interior: 4.0617e-18\n",
      "Epoch 52/601\n",
      "10/10 [==============================] - 0s 49ms/step - loss: 3.1311e-16 - loss initial: 3.0788e-16 - loss interior: 5.2277e-18\n",
      "Epoch 53/601\n",
      "10/10 [==============================] - 1s 50ms/step - loss: 3.0543e-16 - loss initial: 3.0135e-16 - loss interior: 4.0745e-18\n",
      "Epoch 54/601\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.8473e-16 - loss initial: 1.8368e-16 - loss interior: 1.0483e-18\n",
      "Epoch 55/601\n",
      "10/10 [==============================] - 0s 48ms/step - loss: 1.3638e-16 - loss initial: 1.3455e-16 - loss interior: 1.8337e-18\n",
      "Epoch 56/601\n",
      "10/10 [==============================] - 1s 53ms/step - loss: 1.4165e-16 - loss initial: 1.4018e-16 - loss interior: 1.4653e-18\n",
      "Epoch 57/601\n",
      "10/10 [==============================] - 1s 61ms/step - loss: 2.7214e-16 - loss initial: 2.7035e-16 - loss interior: 1.7847e-18\n",
      "Epoch 58/601\n",
      "10/10 [==============================] - 1s 62ms/step - loss: 6.6526e-16 - loss initial: 6.5785e-16 - loss interior: 7.4193e-18\n",
      "Epoch 59/601\n",
      "10/10 [==============================] - 1s 54ms/step - loss: 1.4157e-16 - loss initial: 1.3995e-16 - loss interior: 1.6165e-18\n",
      "Epoch 60/601\n",
      "10/10 [==============================] - 0s 48ms/step - loss: 3.6032e-16 - loss initial: 3.5490e-16 - loss interior: 5.4225e-18\n",
      "Epoch 61/601\n",
      "10/10 [==============================] - 1s 61ms/step - loss: 1.9119e-16 - loss initial: 1.8979e-16 - loss interior: 1.4065e-18\n",
      "Epoch 62/601\n",
      "10/10 [==============================] - 1s 58ms/step - loss: 5.4419e-17 - loss initial: 5.3893e-17 - loss interior: 5.2629e-19\n",
      "Epoch 63/601\n",
      "10/10 [==============================] - 1s 91ms/step - loss: 6.2563e-17 - loss initial: 6.1607e-17 - loss interior: 9.5637e-19\n",
      "Epoch 64/601\n",
      "10/10 [==============================] - 1s 58ms/step - loss: 5.7856e-17 - loss initial: 5.6510e-17 - loss interior: 1.3458e-18\n",
      "Epoch 65/601\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 1.2461e-16 - loss initial: 1.2060e-16 - loss interior: 4.0098e-18\n",
      "Epoch 66/601\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 4.9513e-16 - loss initial: 4.9121e-16 - loss interior: 3.9213e-18\n",
      "Epoch 67/601\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 1.8558e-16 - loss initial: 1.8260e-16 - loss interior: 2.9776e-18\n",
      "Epoch 68/601\n",
      "10/10 [==============================] - 1s 55ms/step - loss: 1.3989e-16 - loss initial: 1.3876e-16 - loss interior: 1.1303e-18\n",
      "Epoch 69/601\n",
      "10/10 [==============================] - 1s 53ms/step - loss: 1.4250e-16 - loss initial: 1.3975e-16 - loss interior: 2.7453e-18\n",
      "Epoch 70/601\n",
      "10/10 [==============================] - 1s 60ms/step - loss: 6.2817e-17 - loss initial: 6.1752e-17 - loss interior: 1.0644e-18\n",
      "Epoch 71/601\n",
      "10/10 [==============================] - 1s 60ms/step - loss: 5.6402e-17 - loss initial: 5.5606e-17 - loss interior: 7.9613e-19\n",
      "Epoch 72/601\n",
      "10/10 [==============================] - 1s 50ms/step - loss: 4.9070e-17 - loss initial: 4.8466e-17 - loss interior: 6.0467e-19\n",
      "Epoch 73/601\n",
      "10/10 [==============================] - 0s 49ms/step - loss: 4.8948e-17 - loss initial: 4.7935e-17 - loss interior: 1.0132e-18\n",
      "Epoch 74/601\n",
      "10/10 [==============================] - 0s 48ms/step - loss: 4.8862e-16 - loss initial: 4.8569e-16 - loss interior: 2.9314e-18\n",
      "Epoch 75/601\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 1.8128e-16 - loss initial: 1.8092e-16 - loss interior: 3.6300e-19\n",
      "Epoch 76/601\n",
      "10/10 [==============================] - 0s 44ms/step - loss: 1.4423e-16 - loss initial: 1.4368e-16 - loss interior: 5.4880e-19\n",
      "Epoch 77/601\n",
      "10/10 [==============================] - 0s 42ms/step - loss: 1.4576e-16 - loss initial: 1.3895e-16 - loss interior: 6.8079e-18\n",
      "Epoch 78/601\n",
      "10/10 [==============================] - 0s 43ms/step - loss: 2.1578e-16 - loss initial: 2.1250e-16 - loss interior: 3.2746e-18\n",
      "Epoch 79/601\n",
      "10/10 [==============================] - 1s 51ms/step - loss: 1.0690e-16 - loss initial: 1.0616e-16 - loss interior: 7.3511e-19\n",
      "Epoch 80/601\n",
      "10/10 [==============================] - 0s 32ms/step - loss: 3.4087e-16 - loss initial: 3.3731e-16 - loss interior: 3.5626e-18\n",
      "Epoch 81/601\n",
      "10/10 [==============================] - 0s 32ms/step - loss: 1.4446e-16 - loss initial: 1.4394e-16 - loss interior: 5.1996e-19\n",
      "Epoch 82/601\n",
      "10/10 [==============================] - 0s 32ms/step - loss: 1.3354e-16 - loss initial: 1.3074e-16 - loss interior: 2.8036e-18\n",
      "Epoch 83/601\n",
      "10/10 [==============================] - 0s 32ms/step - loss: 4.8927e-17 - loss initial: 4.7299e-17 - loss interior: 1.6282e-18\n",
      "Epoch 84/601\n",
      "10/10 [==============================] - 0s 31ms/step - loss: 3.0080e-16 - loss initial: 2.9542e-16 - loss interior: 5.3765e-18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc24ef00280>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(dimension_total,))\n",
    "outputs = create_network(inputs)\n",
    "model = DPDEModel(inputs=inputs, outputs=outputs)\n",
    "batch_generator = DPDEGenerator(n_train, normalised_min=normalised_min, normalised_max=normalised_min, dimension_states=2, dimension_param=4)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(initial_learning_rate))\n",
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "        'loss', patience=50, restore_best_weights=True)\n",
    "    \n",
    "model.fit(x=batch_generator, epochs=nr_epochs, steps_per_epoch=10,callbacks=[callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
