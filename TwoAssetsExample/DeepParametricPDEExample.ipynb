{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "og0T1ieVf-lO"
   },
   "source": [
    "# Deep Parametric PDE for Two Assets\n",
    "\n",
    "Linus Wunderlich, 2020\\\n",
    "jww Kathrin Glau\\\n",
    "Research funded by EPSRC by the New Investigator Award EP/T004738/1.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LWunderlich/DeepPDE/blob/main/TwoAssetsExample/DeepParametricPDEExample.ipynb)\n",
    "\n",
    "Example of my work in progress on the deep parametric PDE method (joint with Kathrin Glau), solving a European basket option with two underlying assets in Black-Scholes model.\n",
    "\n",
    "\n",
    "## Usage: \n",
    "This notebook either loads the trained model used in the article (when load_model==True) or trains a smaller network. \n",
    "### Usage in Google Colab:\n",
    "When training the network using Google Colab it is advised to use a GPU-session, which significantly speeds up the run-times. \n",
    "\n",
    "When you load the network, the notebook tries to automatically download the files. If this fails consider runing the notebook on your computer or loading the model from your google drive. \n",
    "\n",
    "### Usage as a Jupyter-Notebook\n",
    "If you clone the git repository, you have automatically download the model. In this case, the download steps should be skipped and the model is directly loaded.\n",
    "\n",
    "### Common Error Message\n",
    "If you receive an error executing cell 10 reading \n",
    "> OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n",
    "\n",
    "try replacing\n",
    "\n",
    "`data_interior, data_initial = data[0]`\n",
    "\n",
    "in cell [9] by \n",
    "\n",
    "``data_interior, data_initial = data``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3phS6FulxwZ"
   },
   "source": [
    "## Problem Formulation\n",
    "The option price u solves the multivariate Black-Scholes partial differential equation (formulated in the log-price with t being the time to maturity):\n",
    "\n",
    "$\n",
    "\\partial_t u + \n",
    "  r  u \n",
    "  -   \\left(r-\\frac{\\sigma_1^2}{2}\\right)\\partial_{x_1} u \n",
    "  - \\left(r-\\frac{\\sigma_2^2}{2}\\right)\\partial_{x_2} u \n",
    "  -  \\frac{\\sigma_1^2}{2}  \\, \\partial_{x_1x_1} u  \n",
    "  - \\frac{\\sigma_2^2}{2}  \\, \\partial_{x_2x_2} u  \n",
    "  -  \\rho \\sigma_1 \\sigma_2 \\, \\partial_{x_1x_2} u = 0\n",
    "$\n",
    "\n",
    "with the payoff as initial conditions\n",
    "\n",
    "$\n",
    "u(t=0) = \\max(0, (e^{x_1} + e^{x_2})/2 - K).\n",
    "$\n",
    "\n",
    "We can reformulate any option as one with strike price $K=100$, so we keep it fixed.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24AEjb_UISL5"
   },
   "source": [
    " ## Deep Parametric PDE Method\n",
    "\n",
    "The deep parametric PDE method trains a neural network unsupervised to approximate the option price for all values of $t$, $x$ and the option parameters $r, \\sigma_1, \\sigma_2, \\rho$ (collected in the parameter vector $\\mu$):\n",
    "\n",
    "$u_{\\rm{DNN}}(t, x, \\mu) \\approx  u(t, x, \\mu)$.\n",
    "\n",
    "The training is unsupervised as the loss function is purely based on the PDE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.19 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (1.19.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy==1.19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Np2h-iBVlyIh"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow import keras\n",
    "from scipy.stats import norm\n",
    "from numpy.polynomial.hermite import hermgauss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import urllib\n",
    "import os\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GQMRNJ6hcVU"
   },
   "source": [
    "## User settings\n",
    "These can be changed without needing to re-train the network.\n",
    "\n",
    " * load_model: \n",
    "If true, the pre-trained model used in the paper will be downloaded and loaded. If false, a smaller network will be created and trained. Results will be slightly less accurate to allow for a reasonable trainings time\n",
    " * nr_samples_surface_plot: Nuber of points in each direction used for surfaces plots varying the asset prices of the underlyings\n",
    " * nr_samples_scatter_plot: Number of random points (varying time, asset price and option parameters) used for the scatter plots below.\n",
    " * nr_samples_error_calculation: Number of random points used below to approximate the $L^2$ and $L^\\infty$ error of the deep parametric PDE method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "t_ScUmwViffp"
   },
   "outputs": [],
   "source": [
    "load_model = True\n",
    "nr_samples_surface_plot = 21\n",
    "nr_samples_scatter_plot = 1000\n",
    "nr_samples_error_calculation = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxiZh7pzhZ1w"
   },
   "source": [
    "## Model Parameter\n",
    "Re-train model after any changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "6FJmdLurMXwu"
   },
   "outputs": [],
   "source": [
    "# Model parameters. Re-train model after any changes.\n",
    "s_min_interest = 25\n",
    "s_max_interest = 150\n",
    "t_min_interest = 0.5\n",
    "t_max_interest = 4.\n",
    "\n",
    "riskfree_rate_min = 0.1\n",
    "riskfree_rate_max = 0.3\n",
    "riskfree_rate_eval = 0.2\n",
    "\n",
    "volatility_min = 0.1\n",
    "volatility_max = 0.3\n",
    "volatility1_eval = 0.1\n",
    "volatility2_eval = 0.3\n",
    "\n",
    "correlation_min = 0.2\n",
    "correlation_max = 0.8\n",
    "correlation_eval = 0.5\n",
    "\n",
    "strike_price = 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scJKCcNthlAg"
   },
   "source": [
    "## Neural network parameters\n",
    "Settings involving the size and training of the neural network. Re-train after any changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "uscNJrlyhJP0"
   },
   "outputs": [],
   "source": [
    "nr_nodes_per_layer = 90\n",
    "initial_learning_rate = 0.001\n",
    "localisation_parameter = 1/10.\n",
    "\n",
    "n_train = 100\n",
    "nr_epochs = 601"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTwNzRp2hxzr"
   },
   "source": [
    "## Internal parameters\n",
    "These variables should not be changes, unless you are planing to adapt the code to new situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "aAoDQpPPgqbZ"
   },
   "outputs": [],
   "source": [
    "dimension_state = 2\n",
    "dimension_parameter = 4\n",
    "dimension_total = 1 + dimension_state + dimension_parameter\n",
    "\n",
    "t_min = 0.\n",
    "t_max = t_max_interest\n",
    "s_max = strike_price * (1 + 3*volatility_max*t_max)\n",
    "x_max = np.log(s_max)\n",
    "x_min = 2*np.log(strike_price) - x_max\n",
    "\n",
    "normalised_max = 1\n",
    "normalised_min = -1\n",
    "\n",
    "def transform_ab_to_cd(x, a, b, c, d): \n",
    "    \"\"\"\n",
    "    Perform a linear transformation of a scalar from the souce interval\n",
    "    to the target interval.\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- scalar point(s) to transform\n",
    "    a, b -- interval to transform from\n",
    "    c, d -- interval to transform to \n",
    "    \"\"\"\n",
    "    return c + (x-a) * (d-c) / (b-a)\n",
    "\n",
    "def transform_to_logprice(x): \n",
    "    \"\"\" Transform normalised variable to the log-price. \"\"\"\n",
    "    return transform_ab_to_cd(x, normalised_min, normalised_max, x_min, x_max)\n",
    "\n",
    "def transform_to_time(t): \n",
    "    \"\"\" Transform normalised variable to the time variable. \"\"\"\n",
    "    return transform_ab_to_cd(t, normalised_min, normalised_max, t_min, t_max)\n",
    "\n",
    "def normalise_logprice(x):\n",
    "    \"\"\" Transform log-price to its corresponding normalised variable. \"\"\"\n",
    "    return transform_ab_to_cd(x, x_min, x_max, normalised_min, normalised_max)\n",
    "\n",
    "def normalise_time(t): \n",
    "    \"\"\" Transform time to its corresponding normalised variable. \"\"\"\n",
    "    return transform_ab_to_cd(t, t_min, t_max, normalised_min, normalised_max)\n",
    "\n",
    "t_min_interest_normalised = normalise_time(t_min_interest)\n",
    "t_max_interest_normalised = normalise_time(t_max_interest)\n",
    "\n",
    "diff_dx = (normalised_max-normalised_min) / (x_max-x_min) \n",
    "diff_dt = (normalised_max-normalised_min) / (t_max-t_min)\n",
    "\n",
    "def transform_to_riskfree_rate(mu_1):\n",
    "    \"\"\" Transform normalised variable to the risk-free rate. \"\"\"\n",
    "    return transform_ab_to_cd(mu_1, normalised_min, normalised_max,\n",
    "                                    riskfree_rate_min, riskfree_rate_max)\n",
    "\n",
    "def transform_to_volatility(mu_2):\n",
    "    \"\"\" Transform normalised variable to the volatility. \"\"\"\n",
    "    return transform_ab_to_cd(mu_2, normalised_min, normalised_max,\n",
    "                                    volatility_min, volatility_max)\n",
    "    \n",
    "def transform_to_correlation(mu_3):\n",
    "    \"\"\" Transform normalised variable to the correlation. \"\"\"\n",
    "    return transform_ab_to_cd(mu_3, normalised_min, normalised_max,\n",
    "                                    correlation_min, correlation_max)\n",
    "\n",
    "def normalise_riskfree_rate(riskfree_rate):\n",
    "    \"\"\" Transform risk-free rate to its corresponding normalised variable. \"\"\"\n",
    "    return transform_ab_to_cd(riskfree_rate,\n",
    "                              riskfree_rate_min, riskfree_rate_max, \n",
    "                              normalised_min, normalised_max)\n",
    "    \n",
    "def normalise_volatility(volatility):\n",
    "    \"\"\" Transform volatility to its corresponding normalised variable. \"\"\"\n",
    "    return transform_ab_to_cd( volatility, volatility_min, volatility_max,\n",
    "                                            normalised_min, normalised_max)\n",
    "    \n",
    "def normalise_correlation(correlation):\n",
    "    \"\"\" Transform correlation to its corresponding normalised variable. \"\"\"\n",
    "    return transform_ab_to_cd(correlation, correlation_min, correlation_max,\n",
    "                                            normalised_min, normalised_max)\n",
    "\n",
    "\n",
    "riskfree_rate_eval_normalised = normalise_riskfree_rate(riskfree_rate_eval)\n",
    "volatility1_eval_normalised = normalise_volatility(volatility1_eval)\n",
    "volatility2_eval_normalised = normalise_volatility(volatility2_eval)\n",
    "correlation_eval_normalised = normalise_correlation(correlation_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TOAB2SVkQ4j"
   },
   "source": [
    "## Constructing the network\n",
    "The following classes and functions are only necessary to construct and train the network. If the network is loaded, they can be disregarded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "02ogo2SUgO0z"
   },
   "outputs": [],
   "source": [
    "class HighwayLayer(keras.layers.Layer):\n",
    "    \"\"\" Define one layer of the highway network. \"\"\"\n",
    "\n",
    "    def __init__(self, units=50, original_input=dimension_total):\n",
    "        \"\"\" Construct the layer by creating all weights and biases in keras. \"\"\" \n",
    "        super(HighwayLayer, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        # create all weights and biases\n",
    "        self.Uz = self.add_weight(\"Uz\", shape=(original_input, self.units),\n",
    "                                    initializer=\"random_normal\", trainable=True)\n",
    "        self.Ug = self.add_weight(\"Ug\", shape=(original_input, self.units),\n",
    "                                    initializer=\"random_normal\", trainable=True)\n",
    "        self.Ur = self.add_weight(\"Ur\", shape=(original_input, self.units),\n",
    "                                    initializer=\"random_normal\", trainable=True)\n",
    "        self.Uh = self.add_weight(\"Uh\", shape=(original_input, self.units),\n",
    "                                    initializer=\"random_normal\", trainable=True)\n",
    "        \n",
    "        self.Wz = self.add_weight(\"Wz\", shape=(self.units, self.units),\n",
    "                                    initializer=\"random_normal\", trainable=True)\n",
    "        self.Wg = self.add_weight(\"Wg\", shape=(self.units, self.units),\n",
    "                                    initializer=\"random_normal\", trainable=True)\n",
    "        self.Wr = self.add_weight(\"Wr\", shape=(self.units, self.units),\n",
    "                                    initializer=\"random_normal\", trainable=True)\n",
    "        self.Wh = self.add_weight(\"Wh\", shape=(self.units, self.units),\n",
    "                                    initializer=\"random_normal\", trainable=True)\n",
    "        \n",
    "        self.bz = self.add_weight(\"bz\", shape=(self.units,), \n",
    "                                  initializer=\"random_normal\", trainable=True)\n",
    "        self.bg = self.add_weight(\"bg\", shape=(self.units,), \n",
    "                                  initializer=\"random_normal\", trainable=True)\n",
    "        self.br = self.add_weight(\"br\", shape=(self.units,), \n",
    "                                  initializer=\"random_normal\", trainable=True)\n",
    "        self.bh = self.add_weight(\"bh\", shape=(self.units,), \n",
    "                                  initializer=\"random_normal\", trainable=True)\n",
    "\n",
    "    def call(self, input_combined):\n",
    "        \"\"\" Returns the result of the layer calculation.\n",
    "        \n",
    "        Keyord arguments:\n",
    "        input_combined -- Dictionary containing the original input of \n",
    "        the neural network as 'original_variable' and \n",
    "        the output of the previous layer as 'previous layer'.\n",
    "        \"\"\"\n",
    "        previous_layer = input_combined['previous_layer']\n",
    "        original_variable = input_combined['original_variable']\n",
    "\n",
    "        # Evaluate one layer using the weights created by the constructor\n",
    "        Z = tf.keras.activations.tanh(\n",
    "            tf.matmul(original_variable, self.Uz)\n",
    "            + tf.matmul(previous_layer,self.Wz) \n",
    "            + self.bz)\n",
    "        \n",
    "        G = tf.keras.activations.tanh(\n",
    "            tf.matmul(original_variable, self.Ug) \n",
    "            + tf.matmul(previous_layer,self.Wg) \n",
    "            + self.bg)\n",
    "        \n",
    "        R = tf.keras.activations.tanh(\n",
    "            tf.matmul(original_variable, self.Ur) \n",
    "            + tf.matmul(previous_layer,self.Wr) \n",
    "            + self.br)\n",
    "      \n",
    "        SR = tf.multiply(previous_layer, R)\n",
    "\n",
    "        H = tf.keras.activations.tanh(\n",
    "            tf.matmul(original_variable, self.Uh) \n",
    "            + tf.matmul(SR, self.Wh) \n",
    "            + self.bh)\n",
    "        \n",
    "        one_minus_G = tf.ones_like(G) - G\n",
    "\n",
    "        return tf.multiply(one_minus_G, H) + tf.multiply(Z, previous_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMYVGJpADBji"
   },
   "source": [
    "Now we create the network with three highway layers and the additional localisation:\n",
    "$\n",
    "10 \\log( 1 + e^{0.1((e^{x_1} + e^{x_2})/2 - K e^{-rt})})\n",
    "$\n",
    "\n",
    "Note that the network is smaller than \n",
    "  the one loaded, to make the notebook more interactive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "vbBZW-Jyk3ff"
   },
   "outputs": [],
   "source": [
    "def create_network(inputs):\n",
    "    \"\"\" Creates the neural network by creating three highway layers and an \n",
    "    output layer. Returns the output of these layers as a tensorflow variable.\n",
    "\n",
    "    Keyword arguments:\n",
    "    inputs -- Tensorflow variable of the input layer\n",
    "    \"\"\"\n",
    "    layer0 = keras.layers.Dense(nr_nodes_per_layer, activation=\"tanh\")\n",
    "\n",
    "    layer1 = HighwayLayer(units=nr_nodes_per_layer,\n",
    "                          original_input=dimension_total)\n",
    "    layer2 = HighwayLayer(units=nr_nodes_per_layer,\n",
    "                          original_input=dimension_total)\n",
    "    layer3 = HighwayLayer(units=nr_nodes_per_layer,\n",
    "                          original_input=dimension_total)\n",
    "    \n",
    "    last_layer = keras.layers.Dense(1)\n",
    "\n",
    "    outputs_layer0 = layer0(inputs)\n",
    "    outputs_layer1 = layer1({'previous_layer': outputs_layer0, \n",
    "                             'original_variable': inputs})\n",
    "    outputs_layer2 = layer2({'previous_layer': outputs_layer1, \n",
    "                             'original_variable': inputs})\n",
    "    outputs_layer3 = layer3({'previous_layer': outputs_layer2, \n",
    "                             'original_variable': inputs})\n",
    "\n",
    "    outputs_dnn = last_layer(outputs_layer3)\n",
    "    \n",
    "    inputs_t_normalised = inputs[:, 0:1]\n",
    "    inputs_x1_normalised = inputs[:, 1:2]\n",
    "    inputs_x2_normalised = inputs[:, 2:3]\n",
    "    inputs_p1_normalised = inputs[:, 3:4]\n",
    "    \n",
    "    inputs_t = transform_to_time(inputs_t_normalised)\n",
    "    inputs_x1 = transform_to_logprice(inputs_x1_normalised)\n",
    "    inputs_x2 = transform_to_logprice(inputs_x2_normalised)\n",
    "    inputs_s_mean = (tf.math.exp(inputs_x1) + tf.math.exp(inputs_x2))/2.\n",
    "    riskfree_rate = transform_to_riskfree_rate(inputs_p1_normalised)\n",
    "\n",
    "    localisation = tf.math.log(1+tf.math.exp(localisation_parameter * (\n",
    "            inputs_s_mean - strike_price * tf.exp( - riskfree_rate * inputs_t)\n",
    "              )))/localisation_parameter\n",
    "\n",
    "    return outputs_dnn + localisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HssJQGXuVFrO"
   },
   "source": [
    "The Keras generator creates batches of random points in the computational domain to evaluate and train the loss function at. \n",
    "\n",
    "It creates\n",
    " * Points in the interior $(t^i, x^i, \\mu^i)$  as data_train_interior\n",
    " * Points at the payoff $(0, \\hat x^i \\hat \\mu^i)$ as data_train_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GXW4Q0C4bXUj"
   },
   "outputs": [],
   "source": [
    "class DPDEGenerator(keras.utils.Sequence):\n",
    "    \"\"\" Create batches of random points for the network training. \"\"\"\n",
    "\n",
    "    def __init__(self, batch_size):\n",
    "        \"\"\" Initialise the generator by saving the batch size. \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "      \n",
    "    def __len__(self):\n",
    "        \"\"\" Describes the number of points to create \"\"\"\n",
    "        return self.batch_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" Get one batch of random points in the interior of the domain to \n",
    "        train the PDE residual and with initial time to train the initial value.\n",
    "        \"\"\"\n",
    "        data_train_interior = np.random.uniform(\n",
    "            normalised_min, normalised_max, [self.batch_size, dimension_total]) \n",
    "\n",
    "        t_train_initial = normalised_min * np.ones((self.batch_size, 1))\n",
    "        s_and_p_train_initial = np.random.uniform(\n",
    "            normalised_min, normalised_max,\n",
    "            [self.batch_size, dimension_state + dimension_parameter])\n",
    "        \n",
    "        data_train_initial = np.concatenate(\n",
    "            (t_train_initial, s_and_p_train_initial), axis=1)\n",
    "\n",
    "        return [data_train_interior, data_train_initial]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTXdzn1pUlqJ"
   },
   "source": [
    "The Keras model constructs the PDE-based residual for the training. \n",
    "\n",
    "$\\mathcal{J}(u) = \\mathcal{J}_{\\rm{int}}(u) + \\mathcal{J}_{\\rm{ic}}(u)$,\n",
    "\n",
    "with the residual of the PDE\n",
    "\n",
    "$\\mathcal{J}_{\\rm{int}}(u) = \n",
    "\\sum_{i=1}^N\n",
    "\\left(\\partial_t u(t^i, x^i, \\mu^i) + \n",
    "  r  u \n",
    "  -   \\left(r-\\frac{\\sigma_1^2}{2}\\right)\\partial_{x_1} u (t^i, x^i, \\mu^i)\n",
    "  - \\left(r-\\frac{\\sigma_2^2}{2}\\right)\\partial_{x_2} u (t^i, x^i, \\mu^i)\n",
    "  -  \\frac{\\sigma_1^2}{2}  \\, \\partial_{x_1x_1} u  (t^i, x^i, \\mu^i)\n",
    "  - \\frac{\\sigma_2^2}{2}  \\, \\partial_{x_2x_2} u (t^i, x^i, \\mu^i) \n",
    "  -  \\rho \\sigma_1 \\sigma_2 \\, \\partial_{x_1x_2} u(t^i, x^i, \\mu^i) \\right)^2\n",
    "$\n",
    "\n",
    "and the mean-squared error of the initial condition:\n",
    "\n",
    "$\n",
    "\\mathcal{J}_{\\rm{ic}}(u) =\\sum_{i=1}^N \\left(u(0, \\hat x^i, \\hat \\mu^i) -  \\max(0, (e^{\\hat x_1^i} + e^{\\hat x_2^i})/2 - K)\\right)^2.\n",
    "$\n",
    "\n",
    "As all variables were normalised, they and their derivatives need to be transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "rA1RKeotkzoY"
   },
   "outputs": [],
   "source": [
    "class DPDEModel(keras.Model):\n",
    "    \"\"\" Create a keras model with the deep param. PDE loss function \"\"\"\n",
    "\n",
    "    def train_step(self, data):\n",
    "        \"\"\" Create one optimisation stop based on the deep param. PDE loss function. \"\"\"\n",
    "        data_interior, data_initial = data[0]\n",
    "\n",
    "        riskfree_rate_interior = transform_to_riskfree_rate(\n",
    "            data_interior[:, 3:4])\n",
    "        volatility1_interior = transform_to_volatility(data_interior[:, 4:5])\n",
    "        volatility2_interior = transform_to_volatility(data_interior[:, 5:6])\n",
    "        correlation_interior = transform_to_correlation(data_interior[:, 6:7])\n",
    "\n",
    "        x1_initial = transform_to_logprice(data_initial[:, 1:2])\n",
    "        x2_initial = transform_to_logprice(data_initial[:, 2:3])\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            v_interior = self(data_interior, training=True)  # Forward pass\n",
    "            v_initial = self(data_initial, training=True)  # Forward pass bdry\n",
    "\n",
    "            gradient = K.gradients(v_interior, data_interior)[0]\n",
    "\n",
    "            v_dt = diff_dt * gradient[:, 0:1]\n",
    "            v_dx1 = diff_dx * gradient[:, 1:2]\n",
    "            v_dx2 = diff_dx * gradient[:, 2:3]\n",
    "\n",
    "            grad_v_dx1 = K.gradients(v_dx1, data_interior)[0]\n",
    "            grad_v_dx2 = K.gradients(v_dx2, data_interior)[0]\n",
    "\n",
    "            v_dx1dx1 = diff_dx * grad_v_dx1[:, 1:2]\n",
    "            v_dx2dx2 = diff_dx * grad_v_dx2[:, 2:3]\n",
    "            v_dx1dx2 = diff_dx * grad_v_dx1[:, 2:3]\n",
    "            v_dx2dx1 = diff_dx * grad_v_dx2[:, 1:2]\n",
    "\n",
    "            residual_interior = (\n",
    "                v_dt + riskfree_rate_interior * v_interior \n",
    "                - (riskfree_rate_interior - volatility1_interior**2/2) * v_dx1\n",
    "                - (riskfree_rate_interior - volatility2_interior**2/2) * v_dx2\n",
    "                - 0.5 * volatility1_interior**2 * v_dx1dx1\n",
    "                - 0.5 * volatility2_interior**2 * v_dx2dx2 \n",
    "                - 0.5 * correlation_interior \n",
    "                    * volatility1_interior * volatility2_interior * v_dx1dx2 \n",
    "                - 0.5 * correlation_interior \n",
    "                    * volatility2_interior * volatility1_interior * v_dx2dx1\n",
    "                )\n",
    "\n",
    "            s_mean_initial = 0.5 * (\n",
    "                tf.math.exp(x1_initial)+tf.math.exp(x2_initial)) \n",
    "            payoff_initial = K.maximum(s_mean_initial - strike_price, 0)\n",
    "\n",
    "            loss_interior = K.mean(K.square(residual_interior))\n",
    "            loss_initial = K.mean(K.square(v_initial - payoff_initial))\n",
    "            \n",
    "            loss = loss_initial + loss_interior\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        return {\"loss\": loss, \n",
    "                \"loss initial\": loss_initial, \n",
    "                \"loss interior\": loss_interior}\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAMwKy2ilog9"
   },
   "source": [
    "## Train or load model\n",
    "\n",
    "If load_model is set to True, the pre-trained model will be downloaded and imported. If you made any changes to the model or the architecture, you can set load_model to False and re-train. As the training takes a long time in a Google Colab notebook, a smaller version of the network will be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YrLqKY8chkRk",
    "outputId": "c902aa23-c632-478c-e982-15376ebbeed5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-25 22:54:17.882494: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-25 22:54:17.923622: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fe60914fa70 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-04-25 22:54:17.923650: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/601\n",
      "10/10 [==============================] - 0s 47ms/step - loss: 2.8990 - loss initial: 2.3913 - loss interior: 0.5077\n",
      "Epoch 2/601\n",
      "10/10 [==============================] - 0s 44ms/step - loss: 2.6444 - loss initial: 2.1447 - loss interior: 0.4997\n",
      "Epoch 3/601\n",
      "10/10 [==============================] - 0s 45ms/step - loss: 2.5535 - loss initial: 2.1474 - loss interior: 0.4062\n",
      "Epoch 4/601\n",
      "10/10 [==============================] - 0s 46ms/step - loss: 2.2618 - loss initial: 1.8763 - loss interior: 0.3856\n",
      "Epoch 5/601\n",
      "10/10 [==============================] - 0s 45ms/step - loss: 2.1023 - loss initial: 1.7592 - loss interior: 0.3431\n",
      "Epoch 6/601\n",
      "10/10 [==============================] - 0s 42ms/step - loss: 1.8018 - loss initial: 1.3988 - loss interior: 0.4030\n",
      "Epoch 7/601\n",
      "10/10 [==============================] - 1s 53ms/step - loss: 1.8109 - loss initial: 1.4576 - loss interior: 0.3533\n",
      "Epoch 8/601\n",
      "10/10 [==============================] - 1s 55ms/step - loss: 1.5776 - loss initial: 1.1622 - loss interior: 0.4154\n",
      "Epoch 9/601\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 1.1581 - loss initial: 0.8348 - loss interior: 0.3233\n",
      "Epoch 10/601\n",
      "10/10 [==============================] - 1s 53ms/step - loss: 0.8913 - loss initial: 0.6621 - loss interior: 0.2292\n",
      "Epoch 11/601\n",
      "10/10 [==============================] - 0s 47ms/step - loss: 0.9919 - loss initial: 0.6922 - loss interior: 0.2998\n",
      "Epoch 12/601\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.7282 - loss initial: 0.4976 - loss interior: 0.2306\n",
      "Epoch 13/601\n",
      "10/10 [==============================] - 1s 99ms/step - loss: 0.8282 - loss initial: 0.3409 - loss interior: 0.4873\n",
      "Epoch 14/601\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4983 - loss initial: 0.2750 - loss interior: 0.2233\n",
      "Epoch 15/601\n",
      "10/10 [==============================] - 1s 56ms/step - loss: 0.4786 - loss initial: 0.2268 - loss interior: 0.2517\n",
      "Epoch 16/601\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.5072 - loss initial: 0.2181 - loss interior: 0.2890\n",
      "Epoch 17/601\n",
      "10/10 [==============================] - 0s 49ms/step - loss: 0.4157 - loss initial: 0.1668 - loss interior: 0.2488\n",
      "Epoch 18/601\n",
      "10/10 [==============================] - 1s 58ms/step - loss: 0.3417 - loss initial: 0.1827 - loss interior: 0.1590\n",
      "Epoch 19/601\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.3818 - loss initial: 0.1861 - loss interior: 0.1958\n",
      "Epoch 20/601\n",
      "10/10 [==============================] - 0s 46ms/step - loss: 0.3513 - loss initial: 0.2028 - loss interior: 0.1485\n",
      "Epoch 21/601\n",
      "10/10 [==============================] - 1s 53ms/step - loss: 0.3272 - loss initial: 0.1527 - loss interior: 0.1745\n",
      "Epoch 22/601\n",
      "10/10 [==============================] - 1s 56ms/step - loss: 0.3133 - loss initial: 0.1280 - loss interior: 0.1853\n",
      "Epoch 23/601\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 0.3158 - loss initial: 0.1716 - loss interior: 0.1441\n",
      "Epoch 24/601\n",
      "10/10 [==============================] - 0s 45ms/step - loss: 0.2881 - loss initial: 0.1742 - loss interior: 0.1139\n",
      "Epoch 25/601\n",
      "10/10 [==============================] - 1s 52ms/step - loss: 0.2456 - loss initial: 0.1556 - loss interior: 0.0900\n",
      "Epoch 26/601\n",
      "10/10 [==============================] - 0s 48ms/step - loss: 0.2463 - loss initial: 0.1383 - loss interior: 0.1080\n",
      "Epoch 27/601\n",
      "10/10 [==============================] - 0s 41ms/step - loss: 0.2674 - loss initial: 0.1308 - loss interior: 0.1367\n",
      "Epoch 28/601\n",
      "10/10 [==============================] - 0s 44ms/step - loss: 0.2336 - loss initial: 0.1558 - loss interior: 0.0778\n",
      "Epoch 29/601\n",
      "10/10 [==============================] - 0s 49ms/step - loss: 0.2330 - loss initial: 0.1438 - loss interior: 0.0892\n",
      "Epoch 30/601\n",
      "10/10 [==============================] - 0s 47ms/step - loss: 0.2631 - loss initial: 0.1304 - loss interior: 0.1327\n",
      "Epoch 31/601\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.2461 - loss initial: 0.1735 - loss interior: 0.0727\n",
      "Epoch 32/601\n",
      "10/10 [==============================] - 0s 47ms/step - loss: 0.2392 - loss initial: 0.1572 - loss interior: 0.0820\n",
      "Epoch 33/601\n",
      "10/10 [==============================] - 0s 47ms/step - loss: 0.2226 - loss initial: 0.1679 - loss interior: 0.0547\n",
      "Epoch 34/601\n",
      "10/10 [==============================] - 0s 45ms/step - loss: 0.1314 - loss initial: 0.0838 - loss interior: 0.0475\n",
      "Epoch 35/601\n",
      "10/10 [==============================] - 0s 46ms/step - loss: 0.1513 - loss initial: 0.0964 - loss interior: 0.0549\n",
      "Epoch 36/601\n",
      "10/10 [==============================] - 0s 45ms/step - loss: 0.1753 - loss initial: 0.1022 - loss interior: 0.0731\n",
      "Epoch 37/601\n",
      "10/10 [==============================] - 0s 44ms/step - loss: 0.1663 - loss initial: 0.1069 - loss interior: 0.0594\n",
      "Epoch 38/601\n",
      "10/10 [==============================] - 0s 43ms/step - loss: 0.1746 - loss initial: 0.1192 - loss interior: 0.0554\n",
      "Epoch 39/601\n",
      "10/10 [==============================] - 0s 42ms/step - loss: 0.1892 - loss initial: 0.1064 - loss interior: 0.0828\n",
      "Epoch 40/601\n",
      "10/10 [==============================] - 1s 54ms/step - loss: 0.1507 - loss initial: 0.1077 - loss interior: 0.0429\n",
      "Epoch 41/601\n",
      "10/10 [==============================] - 0s 44ms/step - loss: 0.1345 - loss initial: 0.0907 - loss interior: 0.0438\n",
      "Epoch 42/601\n",
      "10/10 [==============================] - 0s 48ms/step - loss: 0.1578 - loss initial: 0.0922 - loss interior: 0.0656\n",
      "Epoch 43/601\n",
      "10/10 [==============================] - 0s 49ms/step - loss: 0.1526 - loss initial: 0.0869 - loss interior: 0.0657\n",
      "Epoch 44/601\n",
      "10/10 [==============================] - 1s 55ms/step - loss: 0.1465 - loss initial: 0.1103 - loss interior: 0.0362\n",
      "Epoch 45/601\n",
      "10/10 [==============================] - 0s 47ms/step - loss: 0.1428 - loss initial: 0.0812 - loss interior: 0.0617\n",
      "Epoch 46/601\n",
      "10/10 [==============================] - 0s 46ms/step - loss: 0.1122 - loss initial: 0.0747 - loss interior: 0.0374\n",
      "Epoch 47/601\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.1493 - loss initial: 0.0955 - loss interior: 0.0538\n",
      "Epoch 48/601\n",
      "10/10 [==============================] - 0s 45ms/step - loss: 0.2158 - loss initial: 0.0982 - loss interior: 0.1176\n",
      "Epoch 49/601\n",
      "10/10 [==============================] - 0s 48ms/step - loss: 0.1483 - loss initial: 0.1144 - loss interior: 0.0339\n",
      "Epoch 50/601\n",
      "10/10 [==============================] - 0s 49ms/step - loss: 0.1437 - loss initial: 0.1020 - loss interior: 0.0417\n",
      "Epoch 51/601\n",
      "10/10 [==============================] - 0s 47ms/step - loss: 0.1433 - loss initial: 0.1089 - loss interior: 0.0344\n",
      "Epoch 52/601\n",
      "10/10 [==============================] - 0s 46ms/step - loss: 0.1515 - loss initial: 0.0776 - loss interior: 0.0739\n",
      "Epoch 53/601\n",
      "10/10 [==============================] - 0s 45ms/step - loss: 0.1086 - loss initial: 0.0833 - loss interior: 0.0254\n",
      "Epoch 54/601\n",
      "10/10 [==============================] - 0s 43ms/step - loss: 0.1136 - loss initial: 0.0824 - loss interior: 0.0312\n",
      "Epoch 55/601\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.1098 - loss initial: 0.0775 - loss interior: 0.0323\n",
      "Epoch 56/601\n",
      "10/10 [==============================] - 1s 52ms/step - loss: 0.1059 - loss initial: 0.0622 - loss interior: 0.0437\n",
      "Epoch 57/601\n",
      "10/10 [==============================] - 0s 46ms/step - loss: 0.1402 - loss initial: 0.0918 - loss interior: 0.0485\n",
      "Epoch 58/601\n",
      "10/10 [==============================] - 0s 46ms/step - loss: 0.1410 - loss initial: 0.1029 - loss interior: 0.0381\n",
      "Epoch 59/601\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.1549 - loss initial: 0.1042 - loss interior: 0.0507\n",
      "Epoch 60/601\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.1505 - loss initial: 0.1082 - loss interior: 0.0423\n",
      "Epoch 61/601\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.1079 - loss initial: 0.0757 - loss interior: 0.0323\n",
      "Epoch 62/601\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.0803 - loss initial: 0.0503 - loss interior: 0.0300\n",
      "Epoch 63/601\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.2398 - loss initial: 0.1383 - loss interior: 0.1015\n",
      "Epoch 64/601\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.2408 - loss initial: 0.2036 - loss interior: 0.0372\n",
      "Epoch 65/601\n",
      "10/10 [==============================] - 1s 61ms/step - loss: 0.1524 - loss initial: 0.1131 - loss interior: 0.0394\n",
      "Epoch 66/601\n",
      "10/10 [==============================] - 1s 53ms/step - loss: 0.1680 - loss initial: 0.0971 - loss interior: 0.0708\n",
      "Epoch 67/601\n",
      "10/10 [==============================] - 0s 47ms/step - loss: 0.1561 - loss initial: 0.1108 - loss interior: 0.0453\n",
      "Epoch 68/601\n",
      "10/10 [==============================] - 1s 58ms/step - loss: 0.1407 - loss initial: 0.1004 - loss interior: 0.0403\n",
      "Epoch 69/601\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.1122 - loss initial: 0.0776 - loss interior: 0.0346\n",
      "Epoch 70/601\n",
      "10/10 [==============================] - 0s 49ms/step - loss: 0.0909 - loss initial: 0.0520 - loss interior: 0.0389\n",
      "Epoch 71/601\n",
      "10/10 [==============================] - 0s 44ms/step - loss: 0.1008 - loss initial: 0.0607 - loss interior: 0.0401\n",
      "Epoch 72/601\n",
      "10/10 [==============================] - 0s 43ms/step - loss: 0.1062 - loss initial: 0.0734 - loss interior: 0.0328\n",
      "Epoch 73/601\n",
      "10/10 [==============================] - 0s 45ms/step - loss: 0.0999 - loss initial: 0.0718 - loss interior: 0.0281\n",
      "Epoch 74/601\n",
      "10/10 [==============================] - 0s 44ms/step - loss: 0.1366 - loss initial: 0.0889 - loss interior: 0.0477\n",
      "Epoch 75/601\n",
      "10/10 [==============================] - 0s 49ms/step - loss: 0.1007 - loss initial: 0.0812 - loss interior: 0.0194\n",
      "Epoch 76/601\n",
      "10/10 [==============================] - 1s 51ms/step - loss: 0.0961 - loss initial: 0.0493 - loss interior: 0.0467\n",
      "Epoch 77/601\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 0.1297 - loss initial: 0.0899 - loss interior: 0.0398\n",
      "Epoch 78/601\n",
      "10/10 [==============================] - 0s 45ms/step - loss: 0.2059 - loss initial: 0.1630 - loss interior: 0.0429\n",
      "Epoch 79/601\n",
      "10/10 [==============================] - 0s 47ms/step - loss: 0.1590 - loss initial: 0.1189 - loss interior: 0.0401\n",
      "Epoch 80/601\n",
      "10/10 [==============================] - 0s 42ms/step - loss: 0.1761 - loss initial: 0.1065 - loss interior: 0.0696\n",
      "Epoch 81/601\n",
      "10/10 [==============================] - 1s 53ms/step - loss: 0.1380 - loss initial: 0.1136 - loss interior: 0.0244\n",
      "Epoch 82/601\n",
      "10/10 [==============================] - 0s 43ms/step - loss: 0.1240 - loss initial: 0.0908 - loss interior: 0.0332\n",
      "Epoch 83/601\n",
      "10/10 [==============================] - 1s 51ms/step - loss: 0.0999 - loss initial: 0.0731 - loss interior: 0.0268\n",
      "Epoch 84/601\n",
      "10/10 [==============================] - 0s 47ms/step - loss: 0.0790 - loss initial: 0.0543 - loss interior: 0.0247\n",
      "Epoch 85/601\n",
      "10/10 [==============================] - 0s 49ms/step - loss: 0.1218 - loss initial: 0.0711 - loss interior: 0.0508\n",
      "Epoch 86/601\n",
      "10/10 [==============================] - 1s 52ms/step - loss: 0.1057 - loss initial: 0.0770 - loss interior: 0.0288\n",
      "Epoch 87/601\n",
      "10/10 [==============================] - 0s 49ms/step - loss: 0.0703 - loss initial: 0.0525 - loss interior: 0.0178\n",
      "Epoch 88/601\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.0794 - loss initial: 0.0543 - loss interior: 0.0251\n",
      "Epoch 89/601\n",
      "10/10 [==============================] - 1s 53ms/step - loss: 0.1080 - loss initial: 0.0771 - loss interior: 0.0310\n",
      "Epoch 90/601\n",
      "10/10 [==============================] - 1s 59ms/step - loss: 0.0915 - loss initial: 0.0668 - loss interior: 0.0247\n",
      "Epoch 91/601\n",
      "10/10 [==============================] - 0s 44ms/step - loss: 0.2005 - loss initial: 0.1381 - loss interior: 0.0623\n",
      "Epoch 92/601\n",
      "10/10 [==============================] - 0s 48ms/step - loss: 0.1154 - loss initial: 0.0916 - loss interior: 0.0238\n",
      "Epoch 93/601\n",
      "10/10 [==============================] - 0s 37ms/step - loss: 0.1197 - loss initial: 0.0807 - loss interior: 0.0390\n",
      "Epoch 94/601\n",
      "10/10 [==============================] - 0s 36ms/step - loss: 0.1015 - loss initial: 0.0715 - loss interior: 0.0300\n",
      "Epoch 95/601\n",
      "10/10 [==============================] - 0s 47ms/step - loss: 0.0682 - loss initial: 0.0499 - loss interior: 0.0183\n",
      "Epoch 96/601\n",
      "10/10 [==============================] - 0s 47ms/step - loss: 0.1249 - loss initial: 0.0870 - loss interior: 0.0379\n",
      "Epoch 97/601\n",
      "10/10 [==============================] - 1s 61ms/step - loss: 0.0903 - loss initial: 0.0639 - loss interior: 0.0264\n",
      "Epoch 98/601\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 0.1021 - loss initial: 0.0649 - loss interior: 0.0372\n",
      "Epoch 99/601\n",
      "10/10 [==============================] - 0s 44ms/step - loss: 0.0707 - loss initial: 0.0462 - loss interior: 0.0245\n",
      "Epoch 100/601\n",
      "10/10 [==============================] - 0s 37ms/step - loss: 0.0553 - loss initial: 0.0423 - loss interior: 0.0130\n",
      "Epoch 101/601\n",
      "10/10 [==============================] - 0s 35ms/step - loss: 0.0566 - loss initial: 0.0358 - loss interior: 0.0208\n",
      "Epoch 102/601\n",
      "10/10 [==============================] - 0s 33ms/step - loss: 0.0584 - loss initial: 0.0366 - loss interior: 0.0219\n",
      "Epoch 103/601\n",
      "10/10 [==============================] - 0s 45ms/step - loss: 0.0837 - loss initial: 0.0517 - loss interior: 0.0320\n",
      "Epoch 104/601\n",
      "10/10 [==============================] - 0s 35ms/step - loss: 0.1019 - loss initial: 0.0617 - loss interior: 0.0402\n",
      "Epoch 105/601\n",
      "10/10 [==============================] - 0s 41ms/step - loss: 0.1084 - loss initial: 0.0844 - loss interior: 0.0240\n",
      "Epoch 106/601\n",
      "10/10 [==============================] - 0s 45ms/step - loss: 0.1261 - loss initial: 0.0863 - loss interior: 0.0397\n",
      "Epoch 107/601\n",
      "10/10 [==============================] - 1s 56ms/step - loss: 0.1440 - loss initial: 0.0909 - loss interior: 0.0532\n",
      "Epoch 108/601\n",
      "10/10 [==============================] - 0s 44ms/step - loss: 0.1207 - loss initial: 0.0972 - loss interior: 0.0235\n",
      "Epoch 109/601\n",
      "10/10 [==============================] - 0s 48ms/step - loss: 0.2011 - loss initial: 0.1045 - loss interior: 0.0967\n",
      "Epoch 110/601\n",
      "10/10 [==============================] - 0s 42ms/step - loss: 0.1565 - loss initial: 0.1207 - loss interior: 0.0357\n",
      "Epoch 111/601\n",
      "10/10 [==============================] - 1s 54ms/step - loss: 0.0994 - loss initial: 0.0708 - loss interior: 0.0286\n",
      "Epoch 112/601\n",
      "10/10 [==============================] - 1s 58ms/step - loss: 0.0723 - loss initial: 0.0564 - loss interior: 0.0159\n",
      "Epoch 113/601\n",
      "10/10 [==============================] - 1s 61ms/step - loss: 0.0939 - loss initial: 0.0669 - loss interior: 0.0270\n",
      "Epoch 114/601\n",
      "10/10 [==============================] - 0s 48ms/step - loss: 0.0877 - loss initial: 0.0694 - loss interior: 0.0183\n",
      "Epoch 115/601\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.0616 - loss initial: 0.0426 - loss interior: 0.0189\n",
      "Epoch 116/601\n",
      "10/10 [==============================] - 0s 50ms/step - loss: 0.1175 - loss initial: 0.0865 - loss interior: 0.0309\n",
      "Epoch 117/601\n",
      "10/10 [==============================] - 0s 42ms/step - loss: 0.1165 - loss initial: 0.0856 - loss interior: 0.0309\n",
      "Epoch 118/601\n",
      "10/10 [==============================] - 0s 44ms/step - loss: 0.0905 - loss initial: 0.0705 - loss interior: 0.0199\n",
      "Epoch 119/601\n",
      "10/10 [==============================] - 0s 42ms/step - loss: 0.1094 - loss initial: 0.0902 - loss interior: 0.0192\n",
      "Epoch 120/601\n",
      "10/10 [==============================] - 1s 58ms/step - loss: 0.0920 - loss initial: 0.0701 - loss interior: 0.0220\n",
      "Epoch 121/601\n",
      "10/10 [==============================] - 1s 52ms/step - loss: 0.0879 - loss initial: 0.0629 - loss interior: 0.0250\n",
      "Epoch 122/601\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 0.0689 - loss initial: 0.0574 - loss interior: 0.0115\n",
      "Epoch 123/601\n",
      "10/10 [==============================] - 1s 54ms/step - loss: 0.0742 - loss initial: 0.0535 - loss interior: 0.0207\n",
      "Epoch 124/601\n",
      "10/10 [==============================] - 0s 47ms/step - loss: 0.0766 - loss initial: 0.0597 - loss interior: 0.0169\n",
      "Epoch 125/601\n",
      "10/10 [==============================] - 1s 53ms/step - loss: 0.1045 - loss initial: 0.0571 - loss interior: 0.0474\n",
      "Epoch 126/601\n",
      "10/10 [==============================] - 0s 39ms/step - loss: 0.0825 - loss initial: 0.0620 - loss interior: 0.0206\n",
      "Epoch 127/601\n",
      "10/10 [==============================] - 1s 56ms/step - loss: 0.0943 - loss initial: 0.0627 - loss interior: 0.0316\n",
      "Epoch 128/601\n",
      "10/10 [==============================] - 1s 61ms/step - loss: 0.1081 - loss initial: 0.0886 - loss interior: 0.0195\n",
      "Epoch 129/601\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.1865 - loss initial: 0.1339 - loss interior: 0.0526\n",
      "Epoch 130/601\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.1124 - loss initial: 0.0898 - loss interior: 0.0226\n",
      "Epoch 131/601\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 0.1541 - loss initial: 0.0868 - loss interior: 0.0673\n",
      "Epoch 132/601\n",
      "10/10 [==============================] - 1s 61ms/step - loss: 0.1113 - loss initial: 0.0869 - loss interior: 0.0244\n",
      "Epoch 133/601\n",
      "10/10 [==============================] - 1s 57ms/step - loss: 0.1520 - loss initial: 0.1003 - loss interior: 0.0517\n",
      "Epoch 134/601\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 0.2007 - loss initial: 0.1661 - loss interior: 0.0347\n",
      "Epoch 135/601\n",
      "10/10 [==============================] - 1s 55ms/step - loss: 0.1264 - loss initial: 0.0980 - loss interior: 0.0285\n",
      "Epoch 136/601\n",
      "10/10 [==============================] - 0s 42ms/step - loss: 0.0954 - loss initial: 0.0659 - loss interior: 0.0294\n",
      "Epoch 137/601\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.0885 - loss initial: 0.0557 - loss interior: 0.0328\n",
      "Epoch 138/601\n",
      "10/10 [==============================] - 0s 49ms/step - loss: 0.1836 - loss initial: 0.1029 - loss interior: 0.0808\n",
      "Epoch 139/601\n",
      "10/10 [==============================] - 0s 34ms/step - loss: 0.1409 - loss initial: 0.1080 - loss interior: 0.0328\n",
      "Epoch 140/601\n",
      "10/10 [==============================] - 0s 31ms/step - loss: 0.0961 - loss initial: 0.0732 - loss interior: 0.0229\n",
      "Epoch 141/601\n",
      "10/10 [==============================] - 0s 33ms/step - loss: 0.0986 - loss initial: 0.0707 - loss interior: 0.0279\n",
      "Epoch 142/601\n",
      "10/10 [==============================] - 0s 33ms/step - loss: 0.1295 - loss initial: 0.0900 - loss interior: 0.0395\n",
      "Epoch 143/601\n",
      "10/10 [==============================] - 0s 32ms/step - loss: 0.0858 - loss initial: 0.0675 - loss interior: 0.0183\n",
      "Epoch 144/601\n",
      "10/10 [==============================] - 0s 33ms/step - loss: 0.0758 - loss initial: 0.0400 - loss interior: 0.0358\n",
      "Epoch 145/601\n",
      "10/10 [==============================] - 0s 32ms/step - loss: 0.1200 - loss initial: 0.0991 - loss interior: 0.0210\n",
      "Epoch 146/601\n",
      "10/10 [==============================] - 0s 33ms/step - loss: 0.1154 - loss initial: 0.0941 - loss interior: 0.0214\n",
      "Epoch 147/601\n",
      "10/10 [==============================] - 0s 32ms/step - loss: 0.0861 - loss initial: 0.0755 - loss interior: 0.0105\n",
      "Epoch 148/601\n",
      "10/10 [==============================] - 0s 44ms/step - loss: 0.0750 - loss initial: 0.0531 - loss interior: 0.0219\n",
      "Epoch 149/601\n",
      "10/10 [==============================] - 0s 37ms/step - loss: 0.0568 - loss initial: 0.0437 - loss interior: 0.0132\n",
      "Epoch 150/601\n",
      "10/10 [==============================] - 0s 32ms/step - loss: 0.0724 - loss initial: 0.0343 - loss interior: 0.0382\n",
      "Epoch 151/601\n",
      "10/10 [==============================] - 0s 39ms/step - loss: 0.0649 - loss initial: 0.0536 - loss interior: 0.0113\n",
      "Epoch 152/601\n",
      "10/10 [==============================] - 0s 31ms/step - loss: 0.0997 - loss initial: 0.0463 - loss interior: 0.0534\n",
      "Epoch 153/601\n",
      "10/10 [==============================] - 0s 32ms/step - loss: 0.0998 - loss initial: 0.0740 - loss interior: 0.0258\n",
      "Epoch 154/601\n",
      "10/10 [==============================] - 0s 32ms/step - loss: 0.0823 - loss initial: 0.0617 - loss interior: 0.0206\n",
      "Epoch 155/601\n",
      "10/10 [==============================] - 0s 31ms/step - loss: 0.0722 - loss initial: 0.0596 - loss interior: 0.0126\n",
      "Epoch 156/601\n",
      "10/10 [==============================] - 0s 31ms/step - loss: 0.0999 - loss initial: 0.0689 - loss interior: 0.0310\n",
      "Epoch 157/601\n",
      "10/10 [==============================] - 0s 37ms/step - loss: 0.0795 - loss initial: 0.0615 - loss interior: 0.0180\n",
      "Epoch 158/601\n",
      "10/10 [==============================] - 0s 32ms/step - loss: 0.0888 - loss initial: 0.0645 - loss interior: 0.0243\n",
      "Epoch 159/601\n",
      "10/10 [==============================] - 0s 33ms/step - loss: 0.0798 - loss initial: 0.0619 - loss interior: 0.0179\n",
      "Epoch 160/601\n",
      "10/10 [==============================] - 0s 44ms/step - loss: 0.0652 - loss initial: 0.0479 - loss interior: 0.0173\n",
      "Epoch 161/601\n",
      "10/10 [==============================] - 0s 43ms/step - loss: 0.0601 - loss initial: 0.0426 - loss interior: 0.0176\n",
      "Epoch 162/601\n",
      "10/10 [==============================] - 1s 56ms/step - loss: 0.0517 - loss initial: 0.0420 - loss interior: 0.0097\n",
      "Epoch 163/601\n",
      "10/10 [==============================] - 0s 38ms/step - loss: 0.0820 - loss initial: 0.0564 - loss interior: 0.0255\n",
      "CPU times: user 2min 30s, sys: 40.2 s, total: 3min 10s\n",
      "Wall time: 2min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "load_model=False\n",
    "if load_model:\n",
    "    # Load model from local folder. If it is not availabe, download it.\n",
    "    os.makedirs('model/variables', exist_ok=True)\n",
    "    url_base = 'https://github.com/LWunderlich/DeepPDE/raw/main/TwoAssetsExample/'\n",
    "    filename = 'model/saved_model.pb'\n",
    "    if not os.path.isfile(filename):\n",
    "        urllib.request.urlretrieve(url_base + filename, filename)\n",
    "        \n",
    "    filename = 'model/variables/variables.data-00000-of-00001'\n",
    "    if not os.path.isfile(filename):\n",
    "        urllib.request.urlretrieve(url_base + filename, filename)\n",
    "\n",
    "    filename = 'model/variables/variables.index'\n",
    "    if not os.path.isfile(filename):\n",
    "        urllib.request.urlretrieve(url_base + filename, filename)\n",
    "\n",
    "    model = keras.models.load_model('model')   \n",
    "else:\n",
    "    # Create and train model from scratch. \n",
    "    inputs = keras.Input(shape=(dimension_total,))\n",
    "    outputs = create_network(inputs)\n",
    "    model = DPDEModel(inputs=inputs, outputs=outputs)\n",
    "    batch_generator = DPDEGenerator(n_train)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(initial_learning_rate))\n",
    "    callback = tf.keras.callbacks.EarlyStopping(\n",
    "        'loss', patience=50, restore_best_weights=True)\n",
    "    \n",
    "    model.fit(x=batch_generator, epochs=nr_epochs, steps_per_epoch=10,\n",
    "                          callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbiGE-Rrlzgi"
   },
   "source": [
    "## Functions for the evaluation\n",
    "In this section, we collect some functions used to evaluate the deep parametric PDE method. This includes a reference pricer and random function evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thrZLW_mlzk0"
   },
   "source": [
    "### Exact solution\n",
    "Compute the exact solution by a 1D-integral of the smoothed payoff using the methods of Bayer, et al (2018) and Ptz (2020).\n",
    "\n",
    "Bayer, C., Siebenmorgen, M. & Tempone, R. (2018), Smoothing the payoff for efficient computation of basket option prices, Quantitative Finance 18(3), 491505.\n",
    "\n",
    "Ptz,C.(2020), Function approximation for option pricing and risk management,\n",
    "PhD thesis, Queen Mary University of London."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RCA6YEk8l0Nt"
   },
   "outputs": [],
   "source": [
    "def decompose_covariance_matrix(t, volatility1, volatility2, correlation):\n",
    "    \"\"\" Decompose covariance matrix as in Lemma 3.1 of Bayer et. al (2018). \"\"\"\n",
    "    sigma_det = (1-correlation**2) * volatility1**2 * volatility2**2\n",
    "    sigma_sum = (volatility1**2 + volatility2**2 \n",
    "                  - 2*correlation*volatility1*volatility2)\n",
    "\n",
    "    ev1 = volatility1**2 - correlation*volatility1*volatility2\n",
    "    ev2 = -(volatility2**2 - correlation*volatility1*volatility2)\n",
    "    ev_norm = np.sqrt(ev1**2 + ev2**2)\n",
    "\n",
    "    eigenvalue = volatility1**2 + volatility2**2 - 2*sigma_det/sigma_sum\n",
    "\n",
    "    v_mat = np.array([ev1, ev2]) / ev_norm\n",
    "    d = t * np.array([sigma_det/sigma_sum, eigenvalue])\n",
    "    return d, v_mat\n",
    "\n",
    "def one_dimensional_exact_solution(\n",
    "        t, s, riskfree_rate, volatility, strike_price):\n",
    "    \"\"\" Standard Black-Scholes formula \"\"\"\n",
    "\n",
    "    d1 = (1 / (volatility*np.sqrt(t))) * (\n",
    "            np.log(s/strike_price) \n",
    "            + (riskfree_rate + volatility**2/2.) * t\n",
    "        )\n",
    "    d2 = d1 - volatility*np.sqrt(t)\n",
    "    return (norm.cdf(d1) * s \n",
    "            - norm.cdf(d2) * strike_price * np.exp(-riskfree_rate*t))\n",
    "\n",
    "def exact_solution(\n",
    "    t, s1, s2, riskfree_rate, volatility1, volatility2, correlation):\n",
    "    \"\"\" Compute the option price of a European basket call option. \"\"\"\n",
    "    if t == 0:\n",
    "        return np.maximum(0.5*(s1+s2) - strike_price, 0)\n",
    "\n",
    "    d, v = decompose_covariance_matrix(\n",
    "        t, volatility1, volatility2, correlation)\n",
    "    \n",
    "    beta = [0.5 * s1 * np.exp(-0.5*t*volatility1**2),\n",
    "            0.5 * s2 * np.exp(-0.5*t*volatility2**2)]\n",
    "    integration_points, integration_weights = hermgauss(33)\n",
    "\n",
    "    # Transform points and weights\n",
    "    integration_points = np.sqrt(2*d[1]) * integration_points.reshape(-1, 1)\n",
    "    integration_weights = integration_weights.reshape(1, -1) / np.sqrt(np.pi)\n",
    "\n",
    "    h_z = (beta[0] * np.exp(v[0]*integration_points)\n",
    "           + beta[1] * np.exp(v[1]*integration_points))\n",
    "\n",
    "    evaluation_at_integration_points = one_dimensional_exact_solution(\n",
    "        t=1, s=h_z * np.exp(0.5*d[0]), \n",
    "        strike_price=np.exp(-riskfree_rate * t) * strike_price, \n",
    "        volatility=np.sqrt(d[0]), riskfree_rate=0.\n",
    "        )\n",
    "    \n",
    "    solution = np.matmul(integration_weights, evaluation_at_integration_points)\n",
    "    \n",
    "    return solution[0, 0]\n",
    "\n",
    "test_solution = exact_solution(t=4., s1=100., s2=100., riskfree_rate=0.2, \n",
    "               volatility1=0.1, volatility2=0.3, correlation=0.5)\n",
    "assert(np.abs(test_solution - 55.096796282039364) < 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CQl6B5M2mKEX"
   },
   "outputs": [],
   "source": [
    "def localisation(t, s1, s2, riskfree_rate=riskfree_rate_eval):\n",
    "    \"\"\" Return the value of the localisation used in the network. \"\"\"\n",
    "    return 1/localisation_parameter * np.log(1 +\n",
    "                    np.exp(localisation_parameter * (\n",
    "                        0.5*(s1+s2) - np.exp(-riskfree_rate*t)*strike_price))\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dpxzRe1fkNbQ"
   },
   "outputs": [],
   "source": [
    "def get_random_points_of_interest(nr_samples, \n",
    "                    t_min_interest=t_min_interest,\n",
    "                    t_max_interest=t_max_interest,\n",
    "                    s_min_interest=s_min_interest,\n",
    "                    s_max_interest=s_max_interest,\n",
    "                    parameter_min_interest_normalised=normalised_min,\n",
    "                    parameter_max_interest_normalised=normalised_max):\n",
    "    \"\"\" Get a number of random points within the defined domain of interest. \"\"\"\n",
    "    t_sample = np.random.uniform(t_min_interest, t_max_interest, \n",
    "                                 [nr_samples, 1])\n",
    "    t_sample_normalised = normalise_time(t_sample)\n",
    "\n",
    "    s_sample = np.random.uniform(\n",
    "        s_min_interest, s_max_interest, [nr_samples, dimension_state])\n",
    "    s1_sample = s_sample[:, 0:1]\n",
    "    s2_sample = s_sample[:, 1:2]\n",
    "    x_sample_normalised = normalise_logprice(np.log(s_sample))\n",
    "\n",
    "    parameter_sample_normalised = np.random.uniform(\n",
    "        normalised_min, normalised_max, [nr_samples, dimension_parameter])\n",
    "    data_normalised = np.concatenate(\n",
    "        (t_sample_normalised, x_sample_normalised, parameter_sample_normalised),\n",
    "        axis=1\n",
    "        )\n",
    "\n",
    "    riskfree_rate_sample = transform_to_riskfree_rate(\n",
    "        parameter_sample_normalised[:, 0])\n",
    "    volatility1_sample = transform_to_volatility(\n",
    "        parameter_sample_normalised[:, 1])\n",
    "    volatility2_sample = transform_to_volatility(\n",
    "        parameter_sample_normalised[:, 2])\n",
    "    correlation_sample = transform_to_correlation(\n",
    "        parameter_sample_normalised[:, 3])\n",
    "    \n",
    "    return data_normalised, t_sample.reshape(-1), s1_sample.reshape(-1), \\\n",
    "            s2_sample.reshape(-1), riskfree_rate_sample, volatility1_sample, \\\n",
    "            volatility2_sample, correlation_sample\n",
    "\n",
    "\n",
    "def get_points_for_plot_at_fixed_time(t_fixed=t_max,\n",
    "                s_min_interest=s_min_interest, s_max_interest=s_max_interest,\n",
    "                riskfree_rate_fixed=riskfree_rate_eval,\n",
    "                volatility1_fixed=volatility1_eval,\n",
    "                volatility2_fixed=volatility2_eval,\n",
    "                correlation_fixed=correlation_eval,\n",
    "                n_plot=nr_samples_surface_plot):\n",
    "    \"\"\" Get the spacial and normalised values for surface plots \n",
    "    at fixed time and parameter, varying both asset prices. \n",
    "    \"\"\"\n",
    "    s1_plot = np.linspace(s_min_interest, s_max_interest, n_plot).reshape(-1,1)\n",
    "    s2_plot = np.linspace(s_min_interest, s_max_interest, n_plot).reshape(-1,1)\n",
    "    [s1_plot_mesh, s2_plot_mesh] = np.meshgrid(s1_plot, s2_plot, indexing='ij')\n",
    "\n",
    "    x1_plot_mesh_normalised = normalise_logprice(\n",
    "        np.log(s1_plot_mesh)).reshape(-1,1)\n",
    "\n",
    "    x2_plot_mesh_normalised = normalise_logprice(\n",
    "        np.log(s2_plot_mesh)).reshape(-1,1)\n",
    "\n",
    "    t_mesh = t_fixed  * np.ones((n_plot**2, 1))\n",
    "    t_mesh_normalised = normalise_time(t_mesh)\n",
    "\n",
    "    parameter1_mesh_normalised = (normalise_riskfree_rate(riskfree_rate_fixed) \n",
    "                                                      * np.ones((n_plot**2, 1)))\n",
    "    parameter2_mesh_normalised = (normalise_volatility(volatility1_fixed) \n",
    "                                                      * np.ones((n_plot**2, 1)))\n",
    "    parameter3_mesh_normalised = (normalise_volatility(volatility2_fixed) \n",
    "                                                      * np.ones((n_plot**2, 1)))\n",
    "    parameter4_mesh_normalised = (normalise_correlation(correlation_fixed) \n",
    "                                                      * np.ones((n_plot**2, 1)))\n",
    "\n",
    "    x_plot_normalised = np.concatenate((t_mesh_normalised,\n",
    "                                        x1_plot_mesh_normalised,\n",
    "                                        x2_plot_mesh_normalised,\n",
    "                                        parameter1_mesh_normalised, \n",
    "                                        parameter2_mesh_normalised,\n",
    "                                        parameter3_mesh_normalised, \n",
    "                                        parameter4_mesh_normalised), axis=1)\n",
    "\n",
    "    \n",
    "    return s1_plot_mesh, s2_plot_mesh, x_plot_normalised\n",
    "\n",
    "\n",
    "s1_plot_mesh, s2_plot_mesh, x_plot_normalised = \\\n",
    "    get_points_for_plot_at_fixed_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GBYH2JlmFXv"
   },
   "source": [
    "## Model evaluation\n",
    "Here we evaluate the model (either loaded or trained). First we look at fixed parameters and vary the asset prices. Afterwards, we sample the whole time-state-parameter-domain to estimate the overall error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7nH9t1ibm8R7"
   },
   "source": [
    "### Plots at final time, varying the asset prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dgqVEelxhttp"
   },
   "outputs": [],
   "source": [
    "DPDE_solution = model.predict(x_plot_normalised).reshape(\n",
    "    nr_samples_surface_plot, nr_samples_surface_plot)\n",
    "\n",
    "exact_solution_evaluated = [exact_solution(t=t_max, s1=s1[0], s2=s2[0], \n",
    "                                riskfree_rate=riskfree_rate_eval, \n",
    "                                volatility1=volatility1_eval, \n",
    "                                volatility2=volatility2_eval,\n",
    "                                correlation=correlation_eval)\n",
    "                  for s1, s2 in zip(\n",
    "                      s1_plot_mesh.reshape(-1, 1), s2_plot_mesh.reshape(-1, 1))\n",
    "                  \n",
    "                  ]\n",
    "exact_solution_evaluated = np.array(exact_solution_evaluated)\n",
    "exact_solution_evaluated = exact_solution_evaluated.reshape(\n",
    "    nr_samples_surface_plot, nr_samples_surface_plot)\n",
    "\n",
    "localisation_plot = localisation(4., s1_plot_mesh, s2_plot_mesh, riskfree_rate_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "3UadNhGSUvIf",
    "outputId": "c973af20-e42c-4e57-91e6-fe971ff8de4a"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "ax.plot_surface(s1_plot_mesh, s2_plot_mesh, DPDE_solution, cmap='viridis')\n",
    "ax.set_title('DPDE Solution')\n",
    "ax.set_xlabel('$s_1$')\n",
    "ax.set_ylabel('$s_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "G1v_i5UyNRMh",
    "outputId": "9cb54fb2-4376-4625-c624-b276d32fb311"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "error = np.abs(DPDE_solution - exact_solution_evaluated)\n",
    "ax.plot_surface(s1_plot_mesh, s2_plot_mesh, error, cmap='viridis')\n",
    "ax.set_title('Error')\n",
    "ax.set_xlabel('$s_1$')\n",
    "ax.set_ylabel('$s_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "NshvEOPMmqMX",
    "outputId": "3f32f69d-849c-45dd-9bfa-5e877de8a691"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "dnn_part = DPDE_solution - localisation_plot\n",
    "ax.plot_surface(s1_plot_mesh, s2_plot_mesh, dnn_part, cmap='viridis')\n",
    "ax.set_title('DNN part')\n",
    "ax.set_xlabel('$s_1$')\n",
    "ax.set_ylabel('$s_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "VwXU5wiOKAl_",
    "outputId": "400ec750-f740-437b-879e-6df2d7c5afb4"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "ax.plot_surface(s1_plot_mesh, s2_plot_mesh, localisation_plot, cmap='viridis')\n",
    "ax.set_title('Localisation')\n",
    "ax.set_xlabel('$s_1$')\n",
    "ax.set_ylabel('$s_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "mCBiWz2Hms6-",
    "outputId": "f7449ce9-db4f-4872-bc6c-63f5984c1b1c"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "ax.plot_surface(s1_plot_mesh, s2_plot_mesh, exact_solution_evaluated, cmap='viridis')\n",
    "ax.set_title('Exact solution')\n",
    "ax.set_xlabel('$s_1$')\n",
    "ax.set_ylabel('$s_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lANJrrEIm_ye"
   },
   "source": [
    "### Scatter plots, varying time, asset prices and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cwYnxcCVJx2A"
   },
   "outputs": [],
   "source": [
    "data_samples, t_samples, s1_samples, s2_samples, riskfree_rate_samples, \\\n",
    "  volatility1_samples, volatility2_samples, correlation_samples = \\\n",
    "              get_random_points_of_interest(nr_samples_scatter_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RUJBu5_uV6gb",
    "outputId": "b0f9361b-9553-4f30-cf33-0b2545a6aad1"
   },
   "outputs": [],
   "source": [
    "print('Predict {} values and measure the time:'.format(nr_samples_scatter_plot))\n",
    "%time DPDE_solution = model.predict(data_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vfbIGWbSXxis"
   },
   "outputs": [],
   "source": [
    "exact_solution_evaluated = [exact_solution(t=t, s1=s1, s2=s2,\n",
    "                                  riskfree_rate=riskfree_rate,\n",
    "                                  volatility1=volatility1,\n",
    "                                  volatility2=volatility2,\n",
    "                                  correlation=correlation\n",
    "                                  ) \n",
    "                  for t, s1, s2, riskfree_rate, volatility1, volatility2,\n",
    "                      correlation \n",
    "                  in zip(t_samples, s1_samples, s2_samples, riskfree_rate_samples, \n",
    "                          volatility1_samples, volatility2_samples,\n",
    "                          correlation_samples)]\n",
    "\n",
    "exact_solution_evaluated = np.array(exact_solution_evaluated).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "vW5BSWnZarfD",
    "outputId": "a69dccfe-5612-4783-80fd-d38a907738f9"
   },
   "outputs": [],
   "source": [
    "plt.scatter(exact_solution_evaluated, DPDE_solution)\n",
    "plt.xlabel('Exact Solution')\n",
    "plt.ylabel('DPDE Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "HKri7gBQgDRt",
    "outputId": "4ebb2e38-6392-492f-c0c0-a631a4bb6ac2"
   },
   "outputs": [],
   "source": [
    "plt.scatter(exact_solution_evaluated, \n",
    "            np.abs(exact_solution_evaluated - DPDE_solution))\n",
    "plt.xlabel('Exact Solution')\n",
    "plt.ylabel('DPDE Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXqDxFSAwrhB"
   },
   "source": [
    "###Estimate the error based on random points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DMt40VPLwrpy"
   },
   "outputs": [],
   "source": [
    "data_samples, t_samples, s1_samples, s2_samples, riskfree_rate_samples, \\\n",
    "  volatility1_samples, volatility2_samples, correlation_samples = \\\n",
    "              get_random_points_of_interest(nr_samples_error_calculation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MtDOXVmlwxCD",
    "outputId": "c6afaaa9-1aa3-46b0-e1d3-adab49988233"
   },
   "outputs": [],
   "source": [
    "print('Predict {} values and measure the time:'.format(nr_samples_error_calculation))\n",
    "%time DPDE_solution = model.predict(data_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vf2MVuVTwyvn"
   },
   "outputs": [],
   "source": [
    "exact_solution_evaluated = [exact_solution(t=t, s1=s1, s2=s2, \n",
    "                                  riskfree_rate=riskfree_rate,\n",
    "                                  volatility1=volatility1,\n",
    "                                  volatility2=volatility2,\n",
    "                                  correlation=correlation) \n",
    "            for t, s1, s2, riskfree_rate, volatility1, volatility2, correlation \n",
    "            in zip(t_samples, s1_samples, s2_samples, riskfree_rate_samples, \n",
    "                    volatility1_samples, volatility2_samples,\n",
    "                    correlation_samples)]\n",
    "\n",
    "exact_solution_evaluated = np.array(exact_solution_evaluated).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4I5tSd0YbBRv",
    "outputId": "d73a3371-947a-434b-bd59-9f560be08a37"
   },
   "outputs": [],
   "source": [
    "print('Estimated MSE Error:')\n",
    "print(np.sqrt(np.mean(np.square(exact_solution_evaluated - DPDE_solution))))\n",
    "\n",
    "print('Relative Error to L2 Norm in %:')\n",
    "print(np.sqrt(np.mean(np.square(exact_solution_evaluated - DPDE_solution))) \n",
    "        / np.sqrt(np.mean(np.square(exact_solution_evaluated)))*100)\n",
    "\n",
    "print('Maximal Error:')\n",
    "print(np.max(exact_solution_evaluated - DPDE_solution))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Train or Load DGM with two underlyings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
